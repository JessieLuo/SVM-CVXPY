{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8500 entries, 0 to 8499\n",
      "Columns: 201 entries, 0 to 200\n",
      "dtypes: float64(201)\n",
      "memory usage: 13.0 MB\n"
     ]
    }
   ],
   "source": [
    "# === Load and prepare data ===\n",
    "train_df = pd.read_csv('./data/train.csv', header=None)\n",
    "test_df = pd.read_csv('./data/test.csv', header=None)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split label and features\n",
    "\n",
    "y_all = train_df.iloc[:, 0].values\n",
    "X_all = train_df.iloc[:, 1:].values\n",
    "y_test = test_df.iloc[:, 0].values\n",
    "X_test = test_df.iloc[:, 1:].values\n",
    "\n",
    "# Convert labels from {0,1} to {-1,1}\n",
    "y_all = 2 * y_all - 1\n",
    "y_test = 2 * y_test - 1\n",
    "\n",
    "# Split into training (first 4000) and validation (rest)\n",
    "X_train = X_all[:4000]\n",
    "y_train = y_all[:4000]\n",
    "X_val = X_all[4000:]\n",
    "y_val = y_all[4000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Primal Soft Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\min_{w,b,\\zeta_i} \\quad \\frac{1}{2}||w||^2_2 + C\\sum_i^m\\zeta_i$\n",
    "\n",
    "$s.t. \\quad y_i(x^Tx+b) \\geq 1 - \\zeta_i \\quad \\zeta_i\\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_primal(data_train, label_train, c_value=1):\n",
    "    N = data_train.shape[0] # number of samples\n",
    "    d = data_train.shape[1] # number of features\n",
    "    \n",
    "    C = cp.Parameter(nonneg=True)\n",
    "    C.value = c_value\n",
    "    w = cp.Variable(d)\n",
    "    b = cp.Variable()\n",
    "    zeta = cp.Variable(N)\n",
    "\n",
    "    # Objective: (1/2)||w||^2 + C/N * \\sum_i^N \\zeta_i\n",
    "    objective = cp.Minimize(0.5 * cp.sum_squares(w) + C/N * cp.sum(zeta))\n",
    "\n",
    "    # Constraints: y_i(w^T x_i + b) â‰¥ 1 - \\zeta_i\n",
    "    constraints = [\n",
    "        cp.multiply(label_train, data_train @ w + b) >= 1 - zeta,\n",
    "        zeta >= 0\n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "\n",
    "    return {'w': w.value, 'b': b.value}\n",
    "\n",
    "def svm_predict_primal(data, labels, model):\n",
    "    w = model['w']\n",
    "    b = model['b']\n",
    "    y_pred = np.sign(data @ w + b)\n",
    "    accuracy = np.mean(y_pred == labels)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias term b: 1.779737104796226\n",
      "Sum of weights np.sum(w): -0.1461666248360982\n",
      "Validation Accuracy: 0.9695555555555555\n",
      "Test Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "# Train SVM on first 4000 samples\n",
    "C = 100\n",
    "model = svm_train_primal(X_train, y_train, C)\n",
    "\n",
    "# Evaluate\n",
    "val_acc = svm_predict_primal(X_val, y_val, model)\n",
    "test_acc = svm_predict_primal(X_test, y_test, model)\n",
    "\n",
    "# Report\n",
    "print(\"Bias term b:\", model['b'])\n",
    "print(\"Sum of weights np.sum(w):\", np.sum(model['w']))\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Dual Soft Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual Soft Margin:\n",
    "\n",
    "$\\max_{\\alpha_i}\\quad \\sum_i\\alpha_i - \\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_jx_ix_j$\n",
    "\n",
    "$s.t. \\quad \\sum_i\\alpha_iy_i=0; 0\\leq\\alpha_i\\leq C$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psd: positive semi-definite matrix, a very important concept to solve quadratic problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4000)\n",
      "(4000, 200)\n",
      "(200, 4000)\n",
      "(4000, 4000)\n"
     ]
    }
   ],
   "source": [
    "# A helper to check if matrix multiplication dimension aligned.\n",
    "\n",
    "r\"\"\"\n",
    "Create a diagonal matrix from the label vector y.\n",
    "This is necessary because NumPy cannot directly use a 1D label array\n",
    "for matrix multiplication because they are viewed as 0-dim.\n",
    "The diagonal matrix allows correct and efficient label-wise multiplication\n",
    "to ensures compatibility with matrix operations.\n",
    "\"\"\"\n",
    "# diag_y = np.diag(y_train)\n",
    "\n",
    "# print(diag_y.shape)\n",
    "# print(X_train.shape)\n",
    "# print(X_train.T.shape)\n",
    "# print(diag_y.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"\"\"\n",
    "verify which format can be multiplied and if the symmetric matrix is PSD\n",
    "even it is not positive, using cvx wrap to forcedly operate such Q.\n",
    "The non-positive PSD is one of potential reason that \n",
    "why dual optimal are not unique but may be multiples\n",
    "\"\"\"\n",
    "# def is_psd(x):\n",
    "#     return np.all(np.linalg.eigvals(x) > 0)\n",
    "# is_psd(diag_y @ X_train @ X_train.T @ diag_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_dual(data_train, label_train, c_value=1):\n",
    "    X = data_train\n",
    "    y = label_train\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "\n",
    "    C = cp.Parameter(nonneg=True)\n",
    "    C.value = c_value\n",
    "    alpha = cp.Variable(N)\n",
    "    Y = np.diag(y)\n",
    "    P = cp.atoms.affine.wraps.psd_wrap(Y @ X @ X.T @ Y) # avoid non-positive Q error\n",
    "\n",
    "    # Define the exact dual objective: \n",
    "    # $$L = \\sum_i^N \\alpha_i - 0.5 * \\sum_{i,j}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j)$$\n",
    "    r\"\"\"\n",
    "    Must convert to Associated symmetric matrix form for the quadratic form \n",
    "    of the dual problem. Any other format calculation, such as original inner sum_squares, \n",
    "    will not work with cvxpy that raising complexity issues and computational cost.\n",
    "    \"\"\"\n",
    "    # $$L = \\sum_i^N \\alpha_i - 0.5 * \\alpha^T P \\alpha$$\n",
    "    obj = cp.sum(alpha) - 0.5 * cp.quad_form(alpha, P)\n",
    "\n",
    "    # Constraints: 0 =< \\alpha_i <= C/N, sum(\\alpha_i, y_i) = 0\n",
    "    constraints = [\n",
    "        alpha >= 0,\n",
    "        alpha <= C / N,\n",
    "        cp.sum(cp.multiply(alpha, y)) == 0\n",
    "    ]\n",
    "\n",
    "    # Solve the problem\n",
    "    prob = cp.Problem(cp.Maximize(obj), constraints)\n",
    "    prob.solve()\n",
    "\n",
    "    alpha_val = alpha.value\n",
    "\n",
    "    return {'alpha': alpha_val, 'optimal': prob.value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Lagrange multipliers (np.sum(alpha)): 7.283433929638309\n"
     ]
    }
   ],
   "source": [
    "# Train SVM on first 4000 samples\n",
    "C = 100\n",
    "model = svm_train_dual(X_train, y_train, C)\n",
    "\n",
    "print(\"Sum of Lagrange multipliers (np.sum(alpha)):\", np.sum(model['alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal $w^*$, we need $w^* = \\sum_i\\alpha_i^*y_ix_i$\n",
    "\n",
    "To find the optimal $b^*$, we need $b^* = y_i - w^{*}x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal sum of w is: -0.14481770546687656\n",
      "Optimal b is: 1.7933270614830095\n"
     ]
    }
   ],
   "source": [
    "# Recover sum of weights and the bias from dual multipliers \\alpha_i\n",
    "\n",
    "diag_y = np.diag(y_train)\n",
    "alpha = model['alpha']\n",
    "\n",
    "w = X_train.T @ diag_y @ alpha\n",
    "b = cp.mean(y_train - X_train @ w)\n",
    "\n",
    "print(f\"Optimal sum of w is: {np.sum(w)}\\nOptimal b is: {b.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't know and never access $\\zeta_i$. To replacement, we use a tolerance $\\epsilon$ (1e-4 or 1e-5 in most situation) for implementation. That is an empirical trick.\n",
    "\n",
    "So, finally, a data is a support vector if and only if it satisfies:\n",
    "\n",
    "$$\\begin{equation*}y_i(wx_i+b) \\leq 1 + \\epsilon\\end{equation*}$$\n",
    "\n",
    "if a data fall into $y_i(wx_i+b)\\geq 1 + \\epsilon$, that would not influence the hyperplane, so they are not support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2688 support vectors\n"
     ]
    }
   ],
   "source": [
    "def get_support_vectors_from_primal(model, X, y, tol=1e-4):\n",
    "    w = model['w']\n",
    "    b = model['b']\n",
    "    \n",
    "    # Calculate margin distances: y_i(wÂ·x_i + b)\n",
    "    margins = y * (X @ w + b)\n",
    "    \n",
    "    # Identify support vectors: points where margin <= 1 + tolerance\n",
    "    support_indices = np.where(margins >= (1 - tol))[0]\n",
    "    support_vectors = X[support_indices]\n",
    "    support_labels = y[support_indices]\n",
    "    \n",
    "    return support_indices, support_vectors, support_labels\n",
    "\n",
    "# Assuming you have training data (X_train, y_train)\n",
    "primal_model = svm_train_primal(X_train, y_train, c_value=1.0)\n",
    "\n",
    "# Get support vectors\n",
    "sv_idx, sv, sv_labels = get_support_vectors_from_primal(\n",
    "    primal_model, X_train, y_train\n",
    ")\n",
    "\n",
    "print(f\"Found {len(sv_idx)} support vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors saved to: ./data/support_vectors.csv\n"
     ]
    }
   ],
   "source": [
    "def save_support_vectors_to_csv(support_indices, support_vectors, support_labels, filename):\n",
    "    df = pd.DataFrame(support_vectors)\n",
    "    df.insert(0, 'label', support_labels)\n",
    "    df.insert(0, 'index', support_indices)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Support vectors saved to: {filename}\")\n",
    "\n",
    "# Save to file\n",
    "save_support_vectors_to_csv(sv_idx, sv, sv_labels, \"./data/support_vectors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dual soft margin, such points are support vector:\n",
    "\n",
    "$\\alpha_i\\geq 0$\n",
    "\n",
    "A hyperplane aspect for support vectors: if we remove support vectors, the $w$ would be changed. If removing the point not leading to $w$ change, they are not support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_vectors_from_dual(model, X: np.ndarray, y: np.ndarray, C, tol=1e-4):\n",
    "    alpha = model['alpha']\n",
    "    C = C\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Identify support vectors (alpha > 0)\n",
    "    support_mask = alpha > tol\n",
    "    support_indices = np.where(support_mask)[0]\n",
    "    support_vectors = X[support_mask]\n",
    "    support_labels = y[support_mask]\n",
    "    \n",
    "    # Classify support vectors into margin vectors and bound vectors\n",
    "    margin_mask = (alpha > tol) & (alpha < C/N - tol)\n",
    "    bound_mask = (alpha > tol) & (alpha >= C/N - tol)\n",
    "    \n",
    "    margin_vectors = np.where(margin_mask)[0]\n",
    "    bound_vectors = np.where(bound_mask)[0]\n",
    "    \n",
    "    return {\n",
    "        'support_indices': support_indices,\n",
    "        'support_vectors': support_vectors,\n",
    "        'support_labels': support_labels,\n",
    "        'margin_vectors': margin_vectors,  # Exactly on the margin\n",
    "        'bound_vectors': bound_vectors    # At the upper bound (potential margin violators)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total support vectors: 392)\n"
     ]
    }
   ],
   "source": [
    "# Get support vectors\n",
    "sv_info = get_support_vectors_from_dual(model, X_train, y_train, C=C)\n",
    "\n",
    "print(f\"Total support vectors: {len(sv_info['support_indices'])})\")\n",
    "# print(\"Classify those support vector type:\")\n",
    "# print(f\"Margin vectors (0 < alpha < C/N): {len(sv_info['margin_vectors'])})\")\n",
    "# print(f\"Bound vectors (alpha = C/N): {len(sv_info['bound_vectors'])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors saved to: ./data/support_vectors_margin.csv\n",
      "Support vectors saved to: ./data/support_vectors_bound.csv\n"
     ]
    }
   ],
   "source": [
    "# Save only margin support vectors\n",
    "save_support_vectors_to_csv(\n",
    "    sv_info['margin_vectors'],                # use margin_vectors as indices\n",
    "    X_train[sv_info['margin_vectors']],       # corresponding feature vectors\n",
    "    y_train[sv_info['margin_vectors']],       # corresponding labels\n",
    "    \"./data/support_vectors_margin.csv\"\n",
    ")\n",
    "\n",
    "# Save only bound support vectors\n",
    "save_support_vectors_to_csv(\n",
    "    sv_info['bound_vectors'],\n",
    "    X_train[sv_info['bound_vectors']],\n",
    "    y_train[sv_info['bound_vectors']],\n",
    "    \"./data/support_vectors_bound.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C candidates: [0.0009765625, 0.00390625, 0.015625, 0.0625, 0.25, 1, 4, 16, 64, 256, 1024]\n",
      "Best C found on validation set: 4\n",
      "Validation accuracy with best C:4: 0.9748888888888889\n",
      "Test accuracy with best C: 0.9746666666666667\n"
     ]
    }
   ],
   "source": [
    "# define regularization C range to find the optimal value\n",
    "regularization_para_C = [2 ** i for i in range(-10, 11, 2)]\n",
    "print(\"C candidates:\", regularization_para_C)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_C = None\n",
    "best_model = None\n",
    "\n",
    "# Start training the SVM and find the best C on validation set\n",
    "for C in regularization_para_C:\n",
    "    svm_model = svm_train_primal(X_train, y_train, C)\n",
    "    val_accuracy = svm_predict_primal(X_val, y_val, svm_model)\n",
    "    test_accuracy = svm_predict_primal(X_test, y_test, svm_model)\n",
    "    # print(f\"Val accuracy with C={C}: {val_accuracy}\")\n",
    "    # print(f\"Test accuracy with C={C}: {test_accuracy}\")\n",
    "    # print()\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_C = C\n",
    "        best_model = svm_model\n",
    "\n",
    "print(f\"Best C found on validation set: {best_C}\")\n",
    "print(f\"Validation accuracy with best C:{best_C}: {best_val_acc}\")\n",
    "# Report test accuracy using the optimal C\n",
    "best_test_acc = svm_predict_primal(X_test, y_test, best_model)\n",
    "print(f\"Test accuracy with best C: {best_test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from libsvm.svmutil import svm_train, svm_predict, svm_problem, svm_parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scikit-learn] Best C: 0.00390625\n",
      "[scikit-learn] Validation accuracy: 0.9666666666666667\n",
      "[scikit-learn] Test accuracy: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define C range as required\n",
    "C_range = [2 ** i for i in range(-10, 11, 2)]\n",
    "\n",
    "# --- scikit-learn SVM ---\n",
    "best_val_acc_sklearn = -1\n",
    "best_C_sklearn = None\n",
    "best_model_sklearn = None\n",
    "\n",
    "for C in C_range:\n",
    "    clf = LinearSVC(C=C, max_iter=10000, dual=True) # may unnecessary care the non-convergence warning; the result may not be influenced. Even increase the max_ite to 100k, the convergence still failed but result always same\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_acc = clf.score(X_val, y_val)\n",
    "    if val_acc > best_val_acc_sklearn:\n",
    "        best_val_acc_sklearn = val_acc\n",
    "        best_C_sklearn = C\n",
    "        best_model_sklearn = clf\n",
    "\n",
    "test_acc_sklearn = best_model_sklearn.score(X_test, y_test)\n",
    "print(f\"[scikit-learn] Best C: {best_C_sklearn}\")\n",
    "print(f\"[scikit-learn] Validation accuracy: {best_val_acc_sklearn}\")\n",
    "print(f\"[scikit-learn] Test accuracy: {test_acc_sklearn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: reaching max number of iterations\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "\n",
      "WARNING: reaching max number of iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIBSVM] Best C: 0.0009765625\n",
      "[LIBSVM] Validation accuracy: 0.9746666666666667\n",
      "[LIBSVM] Test accuracy: 0.9739999999999999\n"
     ]
    }
   ],
   "source": [
    "# --- LIBSVM ---\n",
    "if 'svm_train' in globals():\n",
    "    best_val_acc_libsvm = -1\n",
    "    best_C_libsvm = None\n",
    "    best_model_libsvm = None\n",
    "\n",
    "    for C in C_range:\n",
    "        param = svm_parameter(f'-t 0 -c {C} -q')\n",
    "        prob = svm_problem(list(y_train), X_train.tolist())\n",
    "        model = svm_train(prob, param)\n",
    "        _, val_acc, _ = svm_predict(list(y_val), X_val.tolist(), model, options='-q')\n",
    "        val_acc = val_acc[0] / 100.0  # LIBSVM returns percentage\n",
    "        if val_acc > best_val_acc_libsvm:\n",
    "            best_val_acc_libsvm = val_acc\n",
    "            best_C_libsvm = C\n",
    "            best_model_libsvm = model\n",
    "\n",
    "    # Test accuracy with best C\n",
    "    _, test_acc, _ = svm_predict(list(y_test), X_test.tolist(), best_model_libsvm, options='-q')\n",
    "    test_acc = test_acc[0] / 100.0\n",
    "    print(f\"[LIBSVM] Best C: {best_C_libsvm}\")\n",
    "    print(f\"[LIBSVM] Validation accuracy: {best_val_acc_libsvm}\")\n",
    "    print(f\"[LIBSVM] Test accuracy: {test_acc}\")\n",
    "else:\n",
    "    print(\"LIBSVM not available, skipping LIBSVM SVM classification.\")\n",
    "# run 2min55sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
